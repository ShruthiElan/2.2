1.HDFS
The Hadoop Distributed File System (HDFS) is the primary storage system used by Hadoop applications.
HDFS provides access to data across Hadoop clusters. 
HDFS is a tool for managing big data and supporting big data applications.
When HDFS takes in data, it breaks the information down into separate pieces and distributes them to different nodes 
in a cluster, allowing for parallel processing.

2.hadoop clusters
A Hadoop cluster is a cluster designed for storing and analyzing huge amounts of unstructured data in a distributed
 computing environment. 
 Each HDFS cluster contains the following:
 NameNode: Runs on a “master node” that tracks and directs the storage in cluster.
 DataNode: Runs on “slave nodes,” which make up the majority of the machines within a cluster. The NameNode instructs data
 files to be split into blocks, each of which are replicated three times and stored on machines across the cluster.
 These replicas ensure the entire system won’t go down if one server fails or is taken offline—known as “fault tolerance.”
 Client machine: neither a NameNode or a DataNode, Client machines have Hadoop installed on them.
 They’re responsible for loading data into the cluster, submitting MapReduce jobs and viewing the results of the job 
 once complete.

3.HDFS blocks
Hdfs stores the data in terms of blocks.
the block size in HDFS is very large. 
The default size of HDFS is 64MB. 
The files are split into 64MB blocks and then stored into the hadoop filesystem. 
The hadoop is responsible for distributing the data blocks across multiple nodes.
The blocks are of fixed size, so it is very easy to calculate the number of blocks that can be stored on a disk.
If the size of the file is less than the HDFS block size, then the file does not occupy the complete block storage. 


